{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCL_3EO7A02E",
        "outputId": "6fa3dece-f07f-41d3-b581-d1749e57a4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pot) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from pot) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ot\n",
        "from scipy.optimize.linesearch import scalar_search_armijo\n",
        "from ot.lp import emd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "from collections import deque\n",
        "import copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkwbvA7IoPH",
        "outputId": "b0f283b4-7416-4c25-d2fe-a7a78c6c53ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b3b415ff7417>:3: DeprecationWarning: Please use `scalar_search_armijo` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
            "  from scipy.optimize.linesearch import scalar_search_armijo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# FINAL GFI CODE\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class GraphIntegrator():\n",
        "  # * f_func: function of signature R -> R; either given as lambda or a a pair\n",
        "  #     of two lists ([a_0, a_1,...,a_{t-1}], [b_0, b_1,..., b_{r-1}]). In the\n",
        "  #     latter case, the lists encode a rational function:\n",
        "  #\n",
        "  #        f(x) = (a_0 + a_1 * x + ... + a_{t-1} * x^{t-1}) /\n",
        "  #               (b_0 + b_1 * x + ... + b_{r-1} * x^{r-1})\n",
        "  #\n",
        "  #\n",
        "  # * is_lambda: boolean indicating whether f_func above is given as a lambda\n",
        "  #     or a pair of two lists of coefficients (as described above)\n",
        "  # * graph_adj_lists: the adjacency lists encoding weighted undirected graph:\n",
        "  #     graph_adj_lists[i][j] is a pair of the form (k, w), where k is the id\n",
        "  #     of the jth neighbor of i (we start counting from 0) and w is the weight\n",
        "  #     of an edge connecting i with k. We assume that graph nodes have\n",
        "  #     identifiers: 0, 1, 2, ..., N-1, where N is the number of the nodes of\n",
        "  #     the graph.\n",
        "  def __init__(self, f_func, is_lambda, graph_adj_lists):\n",
        "    self.f_func = f_func\n",
        "    self.is_lambda = is_lambda\n",
        "    self.graph_adj_lists = graph_adj_lists\n",
        "    self.N = len(graph_adj_lists)\n",
        "  # * X_tensor: tensor of the shape: N x b_1 x b_2 x ... b_s, where: N is the\n",
        "  #     number of nodes of the graph and b_1, b_2, ... b_s are sizes of batch\n",
        "  #     dimensions (arbitrary number of them).\n",
        "  # * Output: Tensor Y = einsum(\"mn,n...->m...\", M, X_tensor), where M is the\n",
        "  #     N x N matrix satisfying: M[i][j] ~ f_func(dist(i,j)) and dist is the\n",
        "  #     shortest path distance between i and j in the graph.\n",
        "  def integrate(self, X_tensor):\n",
        "    pass\n",
        "\n",
        "# Auxiliary functions for the brute-force integrator.\n",
        "def compute_shortest_path_distances(graph_adj_lists):\n",
        "  N = len(graph_adj_lists)\n",
        "  edges = np.zeros((N, N))\n",
        "  for i in range(N):\n",
        "    for j, w in graph_adj_lists[i]:\n",
        "      edges[i,j] = w\n",
        "      edges[j,i] = w\n",
        "  csr_adjacency = csr_matrix(edges)\n",
        "  return floyd_warshall(csgraph=csr_adjacency, directed=False)\n",
        "\n",
        "def poly(x, coeff_list):\n",
        "  accum = 0\n",
        "  x_power = 1\n",
        "  for i in range(len(coeff_list)):\n",
        "    accum += x_power * coeff_list[i]\n",
        "    x_power *= x\n",
        "  return accum\n",
        "\n",
        "class BruteForceGraphIntegrator(GraphIntegrator):\n",
        "  def __init__(self, f_func, is_lambda, graph_adj_lists):\n",
        "    super().__init__(f_func, is_lambda, graph_adj_lists)\n",
        "    self.M = compute_shortest_path_distances(self.graph_adj_lists)\n",
        "    for i in range(self.N):\n",
        "      for j in range(self.N):\n",
        "        if not self.is_lambda:\n",
        "          numerator = poly(self.M[i][j], self.f_func[0])\n",
        "          denominator = poly(self.M[i][j], self.f_func[1])\n",
        "          self.M[i][j] = numerator / denominator\n",
        "        else:\n",
        "          self.M[i][j] = self.f_func(self.M[i][j])\n",
        "  def integrate(self, X_tensor):\n",
        "    return np.einsum(\"mn,n...->m...\", self.M, X_tensor)\n",
        "  def get_m_matrix(self):\n",
        "    return self.M\n",
        "\n",
        "# Low-level auxiliary functions for main auxiliary functions:\n",
        "#\n",
        "# 1. integrate_on_tree,\n",
        "# 2. preprocess_tree,\n",
        "# 3. partition_tree,\n",
        "# 4. compute_struct_for_merge.\n",
        "#\n",
        "class CompTree():\n",
        "  def __init__(self, left_child, right_child, left_id_sets, right_id_sets,\n",
        "               left_distances, right_distances, left_ids, right_ids, bfgi):\n",
        "    self.left_child = left_child\n",
        "    self.right_child = right_child\n",
        "    ### Fields containing content.\n",
        "    self.left_id_sets = left_id_sets\n",
        "    self.right_id_sets = right_id_sets\n",
        "    self.left_distances = left_distances\n",
        "    self.right_distances = right_distances\n",
        "    self.left_ids = left_ids\n",
        "    self.right_ids = right_ids\n",
        "    self.bfgi = bfgi\n",
        "\n",
        "def find_vertices(tree, root):\n",
        "  found_vertices = []\n",
        "  parent = np.zeros(len(tree))\n",
        "  parent[root] = -1\n",
        "  queue = deque([root])\n",
        "  while queue:\n",
        "    m = queue.pop()\n",
        "    for neighbour, weight in tree[m]:\n",
        "      if neighbour != parent[m]:\n",
        "        found_vertices.append(neighbour)\n",
        "        queue.append(neighbour)\n",
        "        parent[neighbour] = m\n",
        "  return found_vertices\n",
        "\n",
        "def bfs(graph, start):\n",
        "  visited = np.zeros(len(graph))\n",
        "  distances = np.zeros(len(graph))\n",
        "  queue = deque([])\n",
        "  def bfs_aux(graph, node, visited, distances, queue):\n",
        "    visited[node] = 1\n",
        "    queue.append(node)\n",
        "    while queue:\n",
        "      m = queue.pop()\n",
        "      for neighbour, weight in graph[m]:\n",
        "        if visited[neighbour] == 0:\n",
        "          visited[neighbour] = 1\n",
        "          distances[neighbour] = distances[m] + weight\n",
        "          queue.append(neighbour)\n",
        "    return distances\n",
        "  return bfs_aux(graph, start, visited, distances, queue)\n",
        "\n",
        "def dfs_subtree_sizes(tree, root):\n",
        "  stack = []\n",
        "  sizes = np.zeros(len(tree))\n",
        "  discovered = np.zeros(len(tree))\n",
        "  parent = np.zeros(len(tree), dtype=int)\n",
        "  parent[root] = root\n",
        "  stack.append(root)\n",
        "  while len(stack) > 0:\n",
        "    vertex = stack[-1]\n",
        "    if not discovered[vertex]:\n",
        "      sizes[vertex] += 1\n",
        "      discovered[vertex] = 1\n",
        "      count = 0\n",
        "      for neighbor_weight in tree[vertex]:\n",
        "        neighbor = neighbor_weight[0]\n",
        "        if neighbor != parent[vertex]:\n",
        "          count += 1\n",
        "          stack.append(neighbor)\n",
        "          parent[neighbor] = vertex\n",
        "      if not count:\n",
        "        sizes[parent[vertex]] += sizes[vertex]\n",
        "        x = stack.pop()\n",
        "    else:\n",
        "      if vertex is not root:\n",
        "        sizes[parent[vertex]] += sizes[vertex]\n",
        "      x = stack.pop()\n",
        "  return sizes\n",
        "\n",
        "class Level():\n",
        "  def __init__(self, tf_shape):\n",
        "    self.nodes = []\n",
        "    if tf_shape is not None:\n",
        "      self.tf_value = np.zeros(tf_shape)\n",
        "\n",
        "def compute_cross_contribs(left_distances, left_tf_vals, right_distances,\n",
        "                           right_tf_vals, f_func, is_lambda):\n",
        "  if is_lambda:\n",
        "    left_struct_matrix = np.zeros((len(left_distances), len(right_distances)))\n",
        "    for i in range(len(left_distances)):\n",
        "      for j in range(len(right_distances)):\n",
        "        left_struct_matrix[i][j] = f_func(left_distances[i] + right_distances[j])\n",
        "    right_struct_matrix = np.transpose(left_struct_matrix)\n",
        "    cross_vals_for_left = np.einsum(\"kl,l...->k...\", left_struct_matrix,\n",
        "                                    np.array(right_tf_vals))\n",
        "    cross_vals_for_right = np.einsum(\"lk,k...->l...\", right_struct_matrix,\n",
        "                                     np.array(left_tf_vals))\n",
        "  else:\n",
        "    a, b = f_func\n",
        "    if (len(b) == 1 and b[0] == 1.0):\n",
        "      l_res_shape = tuple([len(left_distances)] +\n",
        "                          list(np.array(right_tf_vals).shape[1:]))\n",
        "      r_res_shape = tuple([len(right_distances)] +\n",
        "                          list(np.array(left_tf_vals).shape[1:]))\n",
        "      cross_vals_for_left = np.zeros(l_res_shape)\n",
        "      cross_vals_for_right = np.zeros(r_res_shape)\n",
        "      for k in range(len(a)):\n",
        "        for b in range(k + 1):\n",
        "          l_array = np.array([np.power(l, b) for l in left_distances])\n",
        "          r_array = np.array([np.power(r, k - b) for r in right_distances])\n",
        "          renorm = a[k] * math.comb(k, b)\n",
        "          cross_vals_for_left += renorm * np.einsum(\"n,m,m...->n...\", l_array,\n",
        "                                                    r_array,\n",
        "                                                    np.array(right_tf_vals))\n",
        "          cross_vals_for_right += renorm * np.einsum(\"m,n,n...->m...\", r_array,\n",
        "                                                     l_array,\n",
        "                                                     np.array(left_tf_vals))\n",
        "\n",
        "  return cross_vals_for_left, cross_vals_for_right\n",
        "\n",
        "# Main auxiliary functions.\n",
        "def partition_tree(original_tree):\n",
        "  tree = copy.deepcopy(original_tree)\n",
        "  root = 0\n",
        "  pivot_point = 0\n",
        "  parent = np.zeros(len(tree), dtype=int)\n",
        "  parent[root] = -1\n",
        "  sizes = dfs_subtree_sizes(tree, root)\n",
        "  queue = deque([root])\n",
        "  while queue:\n",
        "    m = queue.pop()\n",
        "    if sizes[m] > 0.5 * len(tree):\n",
        "      pivot_point = m\n",
        "    for neighbour, _ in tree[m]:\n",
        "      if neighbour != parent[m]:\n",
        "        queue.append(neighbour)\n",
        "        parent[neighbour] = m\n",
        "  sizes[parent[pivot_point]] = len(tree) - sizes[pivot_point]\n",
        "  acc = 0\n",
        "  index = -1\n",
        "  for neighbor, _ in tree[pivot_point]:\n",
        "    if acc > 0.25 * len(tree):\n",
        "      break\n",
        "    else:\n",
        "      acc += sizes[neighbor]\n",
        "      index += 1\n",
        "  left_neighbors = copy.deepcopy(tree[pivot_point][:(index + 1)])\n",
        "  right_neighbors = copy.deepcopy(tree[pivot_point][(index + 1):])\n",
        "  tree[pivot_point] = left_neighbors\n",
        "  left_vertex_set = find_vertices(tree, pivot_point)\n",
        "  if_left_vertex = np.zeros(len(tree), dtype=int)\n",
        "  for elem in left_vertex_set:\n",
        "    if_left_vertex[elem] = 1\n",
        "  left_tree = [left_neighbors]\n",
        "  left_ids = [pivot_point]\n",
        "  right_tree = [right_neighbors]\n",
        "  right_ids = [pivot_point]\n",
        "  for i in range(len(if_left_vertex)):\n",
        "    if i == pivot_point:\n",
        "      continue\n",
        "    if if_left_vertex[i]:\n",
        "      left_tree.append(copy.deepcopy(tree[i]))\n",
        "      left_ids.append(i)\n",
        "    else:\n",
        "      right_tree.append(copy.deepcopy(tree[i]))\n",
        "      right_ids.append(i)\n",
        "  inv_left_ids = np.zeros(len(tree))\n",
        "  inv_right_ids = np.zeros(len(tree))\n",
        "  for i in range(len(left_ids)):\n",
        "    inv_left_ids[left_ids[i]] = i\n",
        "  for i in range(len(right_ids)):\n",
        "    inv_right_ids[right_ids[i]] = i\n",
        "  for i in range(len(left_tree)):\n",
        "    for j in range(len(left_tree[i])):\n",
        "      left_tree[i][j][0] = int(inv_left_ids[left_tree[i][j][0]])\n",
        "  for i in range(len(right_tree)):\n",
        "    for j in range(len(right_tree[i])):\n",
        "      right_tree[i][j][0] = int(inv_right_ids[right_tree[i][j][0]])\n",
        "  return [left_tree, left_ids, right_tree, right_ids]\n",
        "\n",
        "\n",
        "def integrate_cross_terms(left_id_sets, right_id_sets, left_distances,\n",
        "                          right_distances, f_func, is_lambda, X_tensor,\n",
        "                          Y_tensor):\n",
        "  left_tf_vals = []\n",
        "  right_tf_vals = []\n",
        "  for i in range(len(left_id_sets)):\n",
        "    left_tf_vals.append(np.sum(X_tensor[left_id_sets[i],:], axis=0,\n",
        "                               keepdims=False))\n",
        "  for i in range(len(right_id_sets)):\n",
        "    right_tf_vals.append(np.sum(X_tensor[right_id_sets[i],:], axis=0,\n",
        "                                keepdims=False))\n",
        "  res = compute_cross_contribs(left_distances, left_tf_vals, right_distances,\n",
        "                               right_tf_vals, f_func, is_lambda)\n",
        "  cross_vals_for_left = res[0]\n",
        "  cross_vals_for_right = res[1]\n",
        "  for i in range(len(cross_vals_for_left)):\n",
        "    A = cross_vals_for_left[i]\n",
        "    N = len(left_id_sets[i])\n",
        "    fin_shape = tuple([N] + [1] * len(A.shape))\n",
        "    Y_tensor[left_id_sets[i],:] += np.tile(A, fin_shape)\n",
        "  for i in range(len(cross_vals_for_right)):\n",
        "    A = cross_vals_for_right[i]\n",
        "    N = len(right_id_sets[i])\n",
        "    fin_shape = tuple([N] + [1] * len(A.shape))\n",
        "    Y_tensor[right_id_sets[i],:] += np.tile(A, fin_shape)\n",
        "  return Y_tensor\n",
        "\n",
        "def compute_struct_for_merge(left_tree, left_ids, right_tree, right_ids):\n",
        "  left_distances = bfs(left_tree, 0)\n",
        "  right_distances = bfs(right_tree, 0)\n",
        "  left_dict = dict()\n",
        "  right_dict = dict()\n",
        "  for i in range(len(left_distances)):\n",
        "    if left_distances[i] > 0.0:\n",
        "      if left_distances[i] not in left_dict:\n",
        "        left_dict[left_distances[i]] = Level(None)\n",
        "      (left_dict[left_distances[i]].nodes).append(left_ids[i])\n",
        "  for i in range(len(right_distances)):\n",
        "    if right_distances[i] > 0.0:\n",
        "      if right_distances[i] not in right_dict:\n",
        "        right_dict[right_distances[i]] = Level(None)\n",
        "      (right_dict[right_distances[i]].nodes).append(right_ids[i])\n",
        "  left_dict_keys = list(left_dict.keys())\n",
        "  left_dict_nodes = [x.nodes for x in list(left_dict.values())]\n",
        "  right_dict_keys = list(right_dict.keys())\n",
        "  right_dict_nodes = [x.nodes for x in list(right_dict.values())]\n",
        "  return left_dict_keys, left_dict_nodes, right_dict_keys, right_dict_nodes\n",
        "\n",
        "def preprocess_tree(tree, f_func, is_lambda, threshold=6):\n",
        "  if len(tree) < threshold:\n",
        "    bfgi = BruteForceGraphIntegrator(f_func, is_lambda, tree)\n",
        "    return CompTree(None, None, None, None, None, None, None, None, bfgi)\n",
        "  else:\n",
        "    left_tree, left_ids, right_tree, right_ids = partition_tree(tree)\n",
        "    left_child = preprocess_tree(left_tree, f_func, is_lambda, threshold)\n",
        "    right_child = preprocess_tree(right_tree, f_func, is_lambda, threshold)\n",
        "    l_ds, l_ns, r_ds, r_ns = compute_struct_for_merge(left_tree, left_ids,\n",
        "                                                      right_tree, right_ids)\n",
        "    return CompTree(left_child, right_child, l_ns, r_ns, l_ds, r_ds, left_ids,\n",
        "                    right_ids, None)\n",
        "\n",
        "def integrate_on_tree(comp_tree, X_tensor, f_func, is_lambda):\n",
        "  if comp_tree.bfgi is not None:\n",
        "    return comp_tree.bfgi.integrate(X_tensor)\n",
        "  else:\n",
        "    left_result = integrate_on_tree(comp_tree.left_child,\n",
        "                                    X_tensor[comp_tree.left_ids,:], f_func,\n",
        "                                    is_lambda)\n",
        "    right_result = integrate_on_tree(comp_tree.right_child,\n",
        "                                     X_tensor[comp_tree.right_ids,:], f_func,\n",
        "                                     is_lambda)\n",
        "    Y_tensor = np.zeros_like(X_tensor)\n",
        "    Y_tensor[comp_tree.left_ids,:] += left_result\n",
        "    Y_tensor[comp_tree.right_ids,:] += right_result\n",
        "    integrate_cross_terms(comp_tree.left_id_sets, comp_tree.right_id_sets,\n",
        "                          comp_tree.left_distances, comp_tree.right_distances,\n",
        "                          f_func, is_lambda, X_tensor, Y_tensor)\n",
        "    return Y_tensor\n",
        "\n",
        "# Abstract class for the tree maker.\n",
        "\n",
        "class TreeConstructor():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def construct_tree(graph_adj_lists):\n",
        "    pass\n",
        "\n",
        "# Minimum spanning tree functions.\n",
        "\n",
        "class DisjointSet:\n",
        "    parent = {}\n",
        "    size = {}\n",
        "    def makeSet(self, n):\n",
        "      for i in range(n):\n",
        "        self.parent[i] = i\n",
        "        self.size[i] = 1\n",
        "    def find(self, k):\n",
        "      if self.parent[k] == k:\n",
        "        return k\n",
        "      return self.find(self.parent[k])\n",
        "    def union(self, a, b):\n",
        "      x = self.find(a)\n",
        "      y = self.find(b)\n",
        "      if self.size[x] > self.size[y]:\n",
        "        self.parent[y] = x\n",
        "        self.size[x] += self.size[y]\n",
        "      else:\n",
        "        self.parent[x] = y\n",
        "        self.size[y] += self.size[x]\n",
        "\n",
        "def kruskal_algo(graph_adj_lists):\n",
        "  mst = []\n",
        "  tree = []\n",
        "  N = len(graph_adj_lists)\n",
        "  for i in range(N):\n",
        "    tree.append([])\n",
        "  ds = DisjointSet()\n",
        "  ds.makeSet(N)\n",
        "  index = 0\n",
        "  edges = []\n",
        "  for i in range(len(graph_adj_lists)):\n",
        "    for j in range(len(graph_adj_lists[i])):\n",
        "      if graph_adj_lists[i][j][0] < i:\n",
        "        edges.append([i, graph_adj_lists[i][j][0], graph_adj_lists[i][j][1]])\n",
        "  edges.sort(key=lambda x: x[2])\n",
        "  while len(mst) != len(graph_adj_lists) - 1:\n",
        "    src, dest, weight = edges[index]\n",
        "    index = index + 1\n",
        "    x = ds.find(src)\n",
        "    y = ds.find(dest)\n",
        "    if x != y:\n",
        "      tree[src].append([dest, weight])\n",
        "      tree[dest].append([src, weight])\n",
        "      mst.append((src, dest, weight))\n",
        "      ds.union(x, y)\n",
        "  cost = sum([x[2] for x in mst])\n",
        "  return tree\n",
        "\n",
        "class MinimumSpanningTreeConstructor(TreeConstructor):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def construct_tree(self, graph_adj_lists):\n",
        "    return kruskal_algo(graph_adj_lists)\n",
        "\n",
        "class TreeBasedGraphIntegrator(GraphIntegrator):\n",
        "  def __init__(self, f_func, is_lambda, graph_adj_lists, tree_constructor,\n",
        "               threshold=6):\n",
        "    super().__init__(f_func, is_lambda, graph_adj_lists)\n",
        "    self.tree = tree_constructor.construct_tree(graph_adj_lists)\n",
        "    self.comp_tree = preprocess_tree(self.tree, f_func, is_lambda, threshold)\n",
        "  def integrate(self, X_tensor, threshold=6):\n",
        "    return integrate_on_tree(self.comp_tree, X_tensor, self.f_func,\n",
        "                             self.is_lambda)"
      ],
      "metadata": {
        "id": "g5becvC-IfA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StopError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class NonConvergenceError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def solve_1d_linesearch_quad_funct(a, b, c):\n",
        "    # solve min f(x)=a*x**2+b*x+c sur 0,1\n",
        "    f0 = c\n",
        "    df0 = b\n",
        "    f1 = a + f0 + df0\n",
        "\n",
        "    if a > 0:  # convex\n",
        "        minimum = min(1, max(0, -b / (2 * a)))\n",
        "        # print('entrelesdeux')\n",
        "        return minimum\n",
        "    else:  # non convexe donc sur les coins\n",
        "        if f0 > f1:\n",
        "            # print('sur1 f(1)={}'.format(f(1)))\n",
        "            return 1\n",
        "        else:\n",
        "            # print('sur0 f(0)={}'.format(f(0)))\n",
        "            return 0\n",
        "\n",
        "\n",
        "def line_search_armijo(\n",
        "    f,\n",
        "    xk,\n",
        "    pk,\n",
        "    gfk,\n",
        "    old_fval,\n",
        "    args=(),\n",
        "    c1=1e-4,\n",
        "    alpha0=0.99,\n",
        "    alpha_min=None,\n",
        "    alpha_max=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Armijo linesearch function that works with matrices\n",
        "    find an approximate minimum of f(xk+alpha*pk) that satifies the\n",
        "    armijo conditions.\n",
        "    Parameters\n",
        "    ----------\n",
        "    f : function\n",
        "        loss function\n",
        "    xk : np.ndarray\n",
        "        initial position\n",
        "    pk : np.ndarray\n",
        "        descent direction\n",
        "    gfk : np.ndarray\n",
        "        gradient of f at xk\n",
        "    old_fval : float\n",
        "        loss value at xk\n",
        "    args : tuple, optional\n",
        "        arguments given to f\n",
        "    c1 : float, optional\n",
        "        c1 const in armijo rule (>0)\n",
        "    alpha0 : float, optional\n",
        "        initial step (>0)\n",
        "    alpha_min : float, optional\n",
        "        minimum value for alpha\n",
        "    alpha_max : float, optional\n",
        "        maximum value for alpha\n",
        "    Returns\n",
        "    -------\n",
        "    alpha : float\n",
        "        step that satisfy armijo conditions\n",
        "    fc : int\n",
        "        nb of function call\n",
        "    fa : float\n",
        "        loss value at step alpha\n",
        "    \"\"\"\n",
        "    xk = np.atleast_1d(xk)\n",
        "    fc = [0]\n",
        "\n",
        "    def phi(alpha1):\n",
        "        fc[0] += 1\n",
        "        return f(xk + alpha1 * pk, *args)\n",
        "\n",
        "    if old_fval is None:\n",
        "        phi0 = phi(0.0)\n",
        "    else:\n",
        "        phi0 = old_fval\n",
        "\n",
        "    derphi0 = np.sum(pk * gfk)  # Quickfix for matrices\n",
        "    alpha, phi1 = scalar_search_armijo(phi, phi0, derphi0, c1=c1, alpha0=alpha0)\n",
        "\n",
        "    if alpha is None:\n",
        "        return 0.0, fc[0], phi0\n",
        "    else:\n",
        "        if alpha_min is not None or alpha_max is not None:\n",
        "            alpha = np.clip(alpha, alpha_min, alpha_max)\n",
        "        return float(alpha), fc[0], phi1\n",
        "\n",
        "\n",
        "def do_linesearch(\n",
        "    cost,\n",
        "    G,\n",
        "    deltaG,\n",
        "    Mi,\n",
        "    f_val,\n",
        "    armijo=True,\n",
        "    C1=None,\n",
        "    C2=None,\n",
        "    reg=None,\n",
        "    Gc=None,\n",
        "    constC=None,\n",
        "    M=None,\n",
        "    alpha_min=None,\n",
        "    alpha_max=None,\n",
        "    method_type=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Solve the linesearch in the FW iterations\n",
        "    Parameters\n",
        "    ----------\n",
        "    cost : method\n",
        "        The FGW cost\n",
        "    G : ndarray, shape(ns,nt)\n",
        "        The transport map at a given iteration of the FW\n",
        "    deltaG : ndarray (ns,nt)\n",
        "        Difference between the optimal map found by linearization in the FW algorithm and the value at a given iteration\n",
        "    Mi : ndarray (ns,nt)\n",
        "        Cost matrix of the linearized transport problem. Corresponds to the gradient of the cost\n",
        "    f_val :  float\n",
        "        Value of the cost at G\n",
        "    armijo : bool, optionnal\n",
        "            If True the steps of the line-search is found via an armijo research. Else closed form is used.\n",
        "            If there is convergence issues use False.\n",
        "    C1 : ndarray (ns,ns), optionnal\n",
        "        Structure matrix in the source domain. Only used when armijo=False\n",
        "    C2 : ndarray (nt,nt), optionnal\n",
        "        Structure matrix in the target domain. Only used when armijo=False\n",
        "    reg : float, optionnal\n",
        "          Regularization parameter. Corresponds to the alpha parameter of FGW. Only used when armijo=False\n",
        "    Gc : ndarray (ns,nt)\n",
        "        Optimal map found by linearization in the FW algorithm. Only used when armijo=False\n",
        "    constC : ndarray (ns,nt)\n",
        "             Constant for the gromov cost. See [3]. Only used when armijo=False\n",
        "    M : ndarray (ns,nt), optional\n",
        "        Cost matrix between the features. Only used when armijo=False,\n",
        "    Optional:\n",
        "    method_type : str None defaults to brute force\n",
        "    source_integrator : Callable function that does fast matrix multplication for source graph\n",
        "    target_integrator : Callable function that does fast matrix multplication for target graph\n",
        "    Returns\n",
        "    -------\n",
        "    alpha : float\n",
        "            The optimal step size of the FW\n",
        "    fc : useless here\n",
        "    f_val :  float\n",
        "             The value of the cost for the next iteration\n",
        "    References\n",
        "    ----------\n",
        "    .. [3] Vayer Titouan, Chapel Laetitia, Flamary R{\\'e}mi, Tavenard Romain\n",
        "          and Courty Nicolas\n",
        "        \"Optimal Transport for structured data with application on graphs\"\n",
        "        International Conference on Machine Learning (ICML). 2019.\n",
        "    \"\"\"\n",
        "    if armijo:\n",
        "        alpha, fc, f_val = line_search_armijo(\n",
        "            cost, G, deltaG, Mi, f_val, alpha_min=alpha_min, alpha_max=alpha_max\n",
        "        )\n",
        "    else:  # need sym matrices\n",
        "        if method_type is None:\n",
        "            dot = np.dot(np.dot(C1, deltaG), C2)\n",
        "            a = (\n",
        "                -2 * reg * np.sum(dot * deltaG)\n",
        "            )  # -2*alpha*<C1 dt C2,dt> si qqlun est pas bon c'est lui\n",
        "            b = np.sum((M + reg * constC) * deltaG) - 2 * reg * (\n",
        "                np.sum(dot * G) + np.sum(np.dot(np.dot(C1, G), C2) * deltaG)\n",
        "            )\n",
        "            c = cost(G)  # f(xt)\n",
        "\n",
        "        else:\n",
        "            if source_integrator is not None and target_integrator is not None:\n",
        "                partial_dcost = source_integrator.integrate(deltaG)\n",
        "                dot = (\n",
        "                    target_integrator.integrate(partial_dcost.T)\n",
        "                ).T  # use symmetry here\n",
        "                del partial_dcost\n",
        "                a = (\n",
        "                    -2 * reg * np.sum(dot * deltaG)\n",
        "                )  # -2*alpha*<C1 dt C2,dt> si qqlun est pas bon c'est lui\n",
        "                partial_cost = source_integrator.integrate(G)\n",
        "                b1 = (target_integrator.integrate(partial_cost.T)).T\n",
        "                del partial_cost\n",
        "                b = np.sum((M + reg * constC) * deltaG) - 2 * reg * (\n",
        "                    np.sum(dot * G) + np.sum(b1 * deltaG)\n",
        "                )\n",
        "                del b1\n",
        "                c = cost(G)\n",
        "            elif target_integrator is None:\n",
        "                partial_dcost = source_integrator.integrate(deltaG)\n",
        "                dot = np.dot(partial_dcost, C2)\n",
        "                del partial_dcost\n",
        "                a = (\n",
        "                    -2 * reg * np.sum(dot * deltaG)\n",
        "                )  # -2*alpha*<C1 dt C2,dt> si qqlun est pas bon c'est lui\n",
        "                partial_cost = source_integrator.integrate(G)\n",
        "                b1 = np.dot(partial_cost, C2)\n",
        "                del partial_cost\n",
        "                b = np.sum((M + reg * constC) * deltaG) - 2 * reg * (\n",
        "                  np.sum(dot * G) + np.sum(b1 * deltaG)\n",
        "                )\n",
        "                del b1\n",
        "                c = cost(G)\n",
        "            elif source_integrator is None :\n",
        "                partial_dcost = np.dot(C1, deltaG)\n",
        "                dot = (\n",
        "                    target_integrator.integrate(partial_dcost.T)\n",
        "                ).T  # use symmetry here\n",
        "                del partial_dcost\n",
        "                a = (\n",
        "                    -2 * reg * np.sum(dot * deltaG)\n",
        "                )\n",
        "                partial_cost = np.dot(C1,G)\n",
        "                b1 = (target_integrator.integrate(partial_cost.T)).T\n",
        "                del partial_cost\n",
        "                b = np.sum((M + reg * constC) * deltaG) - 2 * reg * (\n",
        "                    np.sum(dot * G) + np.sum(b1 * deltaG)\n",
        "                )\n",
        "                del b1\n",
        "                c = cost(G)\n",
        "\n",
        "        alpha = solve_1d_linesearch_quad_funct(a, b, c)\n",
        "        if alpha_min is not None or alpha_max is not None:\n",
        "            alpha = np.clip(alpha, alpha_min, alpha_max)\n",
        "        fc = None\n",
        "        f_val = cost(G + alpha * deltaG)\n",
        "\n",
        "    return alpha, fc, f_val\n",
        "\n",
        "\n",
        "def cg(\n",
        "    a,\n",
        "    b,\n",
        "    M,\n",
        "    reg,\n",
        "    f,\n",
        "    df,\n",
        "    G0=None,\n",
        "    numItermax=500,\n",
        "    numItermaxEmd=100000,\n",
        "    stopThr=1e-09,\n",
        "    stopThr2=1e-9,\n",
        "    verbose=False,\n",
        "    log=False,\n",
        "    armijo=True,\n",
        "    C1=None,\n",
        "    C2=None,\n",
        "    constC=None,\n",
        "    alpha_min=0.0,\n",
        "    alpha_max=1.0,\n",
        "    method_type=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Solve the general regularized OT problem with conditional gradient\n",
        "        The function solves the following optimization problem:\n",
        "    .. math::\n",
        "        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg*f(\\gamma)\n",
        "        s.t. \\gamma 1 = a\n",
        "             \\gamma^T 1= b\n",
        "             \\gamma\\geq 0\n",
        "    where :\n",
        "    - M is the (ns,nt) metric cost matrix\n",
        "    - :math:`f` is the regularization term ( and df is its gradient)\n",
        "    - a and b are source and target weights (sum to 1)\n",
        "    The algorithm used for solving the problem is conditional gradient as discussed in  [1]_\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : np.ndarray (ns,)\n",
        "        samples weights in the source domain\n",
        "    b : np.ndarray (nt,)\n",
        "        samples in the target domain\n",
        "    M : np.ndarray (ns,nt)\n",
        "        loss matrix\n",
        "    reg : float\n",
        "        Regularization term >0\n",
        "    G0 :  np.ndarray (ns,nt), optional\n",
        "        initial guess (default is indep joint density)\n",
        "    numItermax : int, optional\n",
        "        Max number of iterations\n",
        "    stopThr : float, optional\n",
        "        Stop threshol on error (>0)\n",
        "    verbose : bool, optional\n",
        "        Print information along iterations\n",
        "    log : bool, optional\n",
        "        record log if True\n",
        "    Optional:\n",
        "    method_type : str None defaults to brute force\n",
        "    source_integrator : Callable function that does fast matrix multplication for source graph\n",
        "    target_integrator : Callable function that does fast matrix multplication for target graph\n",
        "    Returns\n",
        "    -------\n",
        "    gamma : (ns x nt) ndarray\n",
        "        Optimal transportation matrix for the given parameters\n",
        "    log : dict\n",
        "        log dictionary return only if log==True in parameters\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Ferradans, S., Papadakis, N., Peyré, G., & Aujol, J. F. (2014). Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3), 1853-1882.\n",
        "    See Also\n",
        "    --------\n",
        "    ot.lp.emd : Unregularized optimal ransport\n",
        "    ot.bregman.sinkhorn : Entropic regularized optimal transport\n",
        "    \"\"\"\n",
        "\n",
        "    loop = 1\n",
        "\n",
        "    if log:\n",
        "        log = {\"loss\": [], \"delta_fval\": []}\n",
        "\n",
        "    if G0 is None:\n",
        "        G = np.outer(a, b)\n",
        "    else:\n",
        "        G = G0\n",
        "\n",
        "    def cost(G):\n",
        "        return np.sum(M * G) + reg * f(G)\n",
        "\n",
        "    f_val = cost(G)  # f(xt)\n",
        "\n",
        "    if log:\n",
        "        log[\"loss\"].append(f_val)\n",
        "\n",
        "    it = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\n",
        "            \"{:5s}|{:12s}|{:8s}\".format(\"It.\", \"Loss\", \"Delta loss\") + \"\\n\" + \"-\" * 32\n",
        "        )\n",
        "        print(\"{:5d}|{:8e}|{:8e}\".format(it, f_val, 0))\n",
        "\n",
        "    while loop:\n",
        "\n",
        "        it += 1\n",
        "        old_fval = f_val\n",
        "        # G=xt\n",
        "        # problem linearization\n",
        "        Mi = M + reg * df(G)  # Gradient(xt)\n",
        "        # set M positive\n",
        "        Mi += Mi.min()\n",
        "\n",
        "        # solve linear program\n",
        "        Gc, logemd = emd(a, b, Mi, numItermax=numItermaxEmd, log=True)  # st\n",
        "\n",
        "        deltaG = Gc - G  # dt\n",
        "\n",
        "        # argmin_alpha f(xt+alpha dt)\n",
        "        alpha, fc, f_val = do_linesearch(\n",
        "            cost=cost,\n",
        "            G=G,\n",
        "            deltaG=deltaG,\n",
        "            Mi=Mi,\n",
        "            f_val=f_val,\n",
        "            armijo=armijo,\n",
        "            constC=constC,\n",
        "            C1=C1,\n",
        "            C2=C2,\n",
        "            reg=reg,\n",
        "            Gc=Gc,\n",
        "            M=M,\n",
        "            alpha_min=alpha_min,\n",
        "            alpha_max=alpha_max,\n",
        "            method_type=method_type,\n",
        "            source_integrator=source_integrator,\n",
        "            target_integrator=target_integrator,\n",
        "        )\n",
        "\n",
        "        if alpha is None or np.isnan(alpha):\n",
        "            raise NonConvergenceError(\"Alpha is not found\")\n",
        "        else:\n",
        "            G = G + alpha * deltaG  # xt+1=xt +alpha dt\n",
        "\n",
        "        # test convergence\n",
        "        if it >= numItermax:\n",
        "            loop = 0\n",
        "\n",
        "        delta_fval = f_val - old_fval\n",
        "        abs_delta_fval = abs(f_val - old_fval)\n",
        "\n",
        "        relative_delta_fval = abs_delta_fval / abs(f_val)\n",
        "        if relative_delta_fval < stopThr or abs_delta_fval < stopThr2:\n",
        "            loop = 0\n",
        "\n",
        "        if log:\n",
        "            log[\"loss\"].append(f_val)\n",
        "            log[\"delta_fval\"].append(delta_fval)\n",
        "\n",
        "        if verbose:\n",
        "            if it % 20 == 0:\n",
        "                print(\n",
        "                    \"{:5s}|{:12s}|{:8s}\".format(\"It.\", \"Loss\", \"Delta loss\")\n",
        "                    + \"\\n\"\n",
        "                    + \"-\" * 32\n",
        "                )\n",
        "            print(\"{:5d}|{:8e}|{:8e}|{:5e}\".format(it, f_val, delta_fval, alpha))\n",
        "\n",
        "    if log:\n",
        "        log.update(logemd)\n",
        "        return G, log\n",
        "    else:\n",
        "        return G"
      ],
      "metadata": {
        "id": "swaV6ZZ-A43b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_multiply_matrix_square(integrator, field):\n",
        "    \"\"\"\n",
        "    Fast mutiplication with Hadamard square of a matrix and a vector\n",
        "    Args : integrator : fast graph field integrator to compute einsum with a vector\n",
        "    \"\"\"\n",
        "    assert field.shape[1] == 1\n",
        "    partial_field = integrator.integrate(np.diag(field.squeeze())).T\n",
        "    return np.diag(integrator.integrate(partial_field)).reshape(-1, 1)\n",
        "\n",
        "\n",
        "def init_matrix(\n",
        "    C1,\n",
        "    C2,\n",
        "    p,\n",
        "    q,\n",
        "    loss_fun=\"square_loss\",\n",
        "    method_type=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "    \"\"\"Return loss matrices and tensors for Gromov-Wasserstein fast computation\n",
        "    Returns the value of \\mathcal{L}(C1,C2) \\otimes T with the selected loss\n",
        "    function as the loss function of Gromow-Wasserstein discrepancy.\n",
        "    The matrices are computed as described in Proposition 1 in [1]\n",
        "    Where :\n",
        "        * C1 : Metric cost matrix in the source space\n",
        "        * C2 : Metric cost matrix in the target space\n",
        "        * T : A coupling between those two spaces\n",
        "    The square-loss function L(a,b)=(1/2)*|a-b|^2 is read as :\n",
        "        L(a,b) = f1(a)+f2(b)-h1(a)*h2(b) with :\n",
        "            * f1(a)=(a^2)\n",
        "            * f2(b)=(b^2)\n",
        "            * h1(a)=a\n",
        "            * h2(b)=2b\n",
        "    Parameters\n",
        "    ----------\n",
        "    C1 : ndarray, shape (ns, ns)\n",
        "         Metric cost matrix in the source space\n",
        "    C2 : ndarray, shape (nt, nt)\n",
        "         Metric costfr matrix in the target space\n",
        "    T :  ndarray, shape (ns, nt)\n",
        "         Coupling between source and target spaces\n",
        "    p : ndarray, shape (ns,)\n",
        "    method_type : (str) Choose one of [None, \"diffusion\", \"separator\"]\n",
        "    source_integrator : Callable , fast graph field integrator for source points\n",
        "    target_integrator : Callable , fast graph field integrator for target points\n",
        "    Returns\n",
        "    -------\n",
        "    constC : ndarray, shape (ns, nt)\n",
        "           Constant C matrix in Eq. (6)\n",
        "    hC1 : ndarray, shape (ns, ns)\n",
        "           h1(C1) matrix in Eq. (6)\n",
        "    hC2 : ndarray, shape (nt, nt)\n",
        "           h2(C) matrix in Eq. (6)\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Peyré, Gabriel, Marco Cuturi, and Justin Solomon,\n",
        "    \"Gromov-Wasserstein averaging of kernel and distance matrices.\"\n",
        "    International Conference on Machine Learning (ICML). 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    if loss_fun == \"square_loss\":\n",
        "\n",
        "        def f1(a):\n",
        "            return a**2\n",
        "\n",
        "        def f2(b):\n",
        "            return b**2\n",
        "\n",
        "        def h1(a):\n",
        "            return a\n",
        "\n",
        "        def h2(b):\n",
        "            return 2 * b\n",
        "\n",
        "    elif loss_fun == \"kl_loss\":\n",
        "\n",
        "        def f1(a):\n",
        "            return a * np.log(a + 1e-15) - a\n",
        "\n",
        "        def f2(b):\n",
        "            return b\n",
        "\n",
        "        def h1(a):\n",
        "            return a\n",
        "\n",
        "        def h2(b):\n",
        "            return np.log(b + 1e-15)\n",
        "\n",
        "    if method_type is None:\n",
        "        constC1 = np.dot(\n",
        "            np.dot(f1(C1), p.reshape(-1, 1)), np.ones(len(q)).reshape(1, -1)\n",
        "        )\n",
        "        constC2 = np.dot(\n",
        "            np.ones(len(p)).reshape(-1, 1), np.dot(q.reshape(1, -1), f2(C2).T)\n",
        "        )\n",
        "\n",
        "    else :\n",
        "        if loss_fun == \"square_loss\":\n",
        "            if source_integrator is not None and target_integrator is not None:\n",
        "                constC1 = np.dot(\n",
        "                    fast_multiply_matrix_square(source_integrator, p.reshape(-1, 1)),\n",
        "                    np.ones(len(q)).reshape(1, -1),\n",
        "                )\n",
        "                constC2 = np.dot(\n",
        "                    np.ones(len(p)).reshape(-1, 1),\n",
        "                    fast_multiply_matrix_square(target_integrator, q.reshape(-1, 1)).T,\n",
        "                )\n",
        "            elif target_integrator is None and source_integrator is not None:\n",
        "                constC1 = np.dot(\n",
        "                    fast_multiply_matrix_square(source_integrator, p.reshape(-1, 1)),\n",
        "                    np.ones(len(q)).reshape(1, -1),\n",
        "                )\n",
        "                constC2 = np.dot(\n",
        "                np.ones(len(p)).reshape(-1, 1), np.dot(q.reshape(1, -1), f2(C2).T)\n",
        "            )\n",
        "            elif source_integrator is None and target_integrator is not None:\n",
        "                constC1 = np.dot(\n",
        "                np.dot(f1(C1), p.reshape(-1, 1)), np.ones(len(q)).reshape(1, -1)\n",
        "            )\n",
        "                constC2 = np.dot(\n",
        "                    np.ones(len(p)).reshape(-1, 1),\n",
        "                    fast_multiply_matrix_square(target_integrator, q.reshape(-1, 1)).T,\n",
        "                )\n",
        "\n",
        "        elif loss_fun == \"kl_loss\":\n",
        "            constC1 = np.dot(\n",
        "                np.dot(f1(C1), p.reshape(-1, 1)), np.ones(len(q)).reshape(1, -1)\n",
        "            )  # no idea how to make it faster\n",
        "            constC2_partial = (\n",
        "                target_integrator.integrate(q.reshape(1, -1).T)\n",
        "            ).T\n",
        "            constC2 = np.dot(np.ones(len(p)).reshape(-1, 1), constC2_partial)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported combination of loss and methods\")\n",
        "\n",
        "    constC = constC1 + constC2\n",
        "\n",
        "    if method_type is None:\n",
        "        hC1 = h1(C1)\n",
        "        hC2 = h2(C2)\n",
        "    else:\n",
        "        if loss_fun == \"square_loss\":\n",
        "            if C1 is None and C2 is None :\n",
        "                hC1, hC2 = None, None\n",
        "            elif C1 is None and C2 is not None :\n",
        "                hC1, hC2 = None, h2(C2)\n",
        "            elif C2 is None and C1 is not None :\n",
        "                hC1, hC2 = h1(C1), None\n",
        "        else:\n",
        "            hC1, hC2 = None, h2(C2)\n",
        "\n",
        "    return constC, hC1, hC2\n",
        "\n",
        "\n",
        "def tensor_product(\n",
        "    constC,\n",
        "    hC1,\n",
        "    hC2,\n",
        "    T,\n",
        "    method_type=None,\n",
        "    loss_fun=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "\n",
        "    \"\"\"Return the tensor for Gromov-Wasserstein fast computation\n",
        "    The tensor is computed as described in Proposition 1 Eq. (6) in [1].\n",
        "    Parameters\n",
        "    ----------\n",
        "    constC : ndarray, shape (ns, nt)\n",
        "           Constant C matrix in Eq. (6)\n",
        "    hC1 : ndarray, shape (ns, ns)\n",
        "           h1(C1) matrix in Eq. (6)\n",
        "    hC2 : ndarray, shape (nt, nt)\n",
        "           h2(C) matrix in Eq. (6)\n",
        "    T : ndarray shape (ns,nt) coupling matrix between source and target\n",
        "    Optional :\n",
        "    method_type : str None defaults to brute force\n",
        "    source_integrator : Callable function that does fast matrix multplication for source graph\n",
        "    target_integrator : Callable function that does fast matrix multplication for target graph\n",
        "    Returns\n",
        "    -------\n",
        "    tens : ndarray, shape (ns, nt)\n",
        "           \\mathcal{L}(C1,C2) \\otimes T tensor-matrix multiplication result\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Peyré, Gabriel, Marco Cuturi, and Justin Solomon,\n",
        "    \"Gromov-Wasserstein averaging of kernel and distance matrices.\"\n",
        "    International Conference on Machine Learning (ICML). 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    if method_type is None:\n",
        "        A = -np.dot(np.dot(hC1, T), hC2.T)\n",
        "    else:\n",
        "        if loss_fun == \"square_loss\":\n",
        "            if source_integrator is not None and target_integrator is not None:\n",
        "                partial_prod = source_integrator.integrate(T)\n",
        "                A = -2 * (target_integrator.integrate(partial_prod.T)).T\n",
        "                del partial_prod\n",
        "            elif source_integrator is not None and target_integrator is None:\n",
        "                partial_prod = source_integrator.integrate(T)\n",
        "                A = -np.dot(partial_prod, hC2.T)\n",
        "                del partial_prod\n",
        "            elif target_integrator is not None and source_integrator is None :\n",
        "                partial_prod = np.dot(hC1, T)\n",
        "                A = -2 * (target_integrator.integrate(partial_prod.T)).T\n",
        "                del partial_prod\n",
        "        elif loss_fun == \"kl_loss\":\n",
        "            partial_prod = source_integrator.integrate(T)\n",
        "            A = -np.dot(partial_prod, hC2.T)\n",
        "            del partial_prod\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Other types of losses are not currently supported.\"\n",
        "            )\n",
        "    tens = constC + A\n",
        "\n",
        "    return tens\n",
        "\n",
        "\n",
        "def gwloss(\n",
        "    constC,\n",
        "    hC1,\n",
        "    hC2,\n",
        "    T,\n",
        "    method_type=None,\n",
        "    loss_fun=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "\n",
        "    \"\"\"Return the Loss for Gromov-Wasserstein\n",
        "    The loss is computed as described in Proposition 1 Eq. (6) in [1].\n",
        "    Parameters\n",
        "    ----------\n",
        "    constC : ndarray, shape (ns, nt)\n",
        "           Constant C matrix in Eq. (6)\n",
        "    hC1 : ndarray, shape (ns, ns)\n",
        "           h1(C1) matrix in Eq. (6)\n",
        "    hC2 : ndarray, shape (nt, nt)\n",
        "           h2(C) matrix in Eq. (6)\n",
        "    T : ndarray, shape (ns, nt)\n",
        "           Current value of transport matrix T\n",
        "    Optional :\n",
        "    method_type : str None defaults to brute force\n",
        "    source_integrator : Callable function that does fast matrix multplication for source graph\n",
        "    target_integrator : Callable function that does fast matrix multplication for target graph\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float\n",
        "           Gromov Wasserstein loss\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Peyré, Gabriel, Marco Cuturi, and Justin Solomon,\n",
        "    \"Gromov-Wasserstein averaging of kernel and distance matrices.\"\n",
        "    International Conference on Machine Learning (ICML). 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    tens = tensor_product(\n",
        "        constC,\n",
        "        hC1,\n",
        "        hC2,\n",
        "        T,\n",
        "        method_type=method_type,\n",
        "        loss_fun=loss_fun,\n",
        "        source_integrator=source_integrator,\n",
        "        target_integrator=target_integrator,\n",
        "    )\n",
        "    return np.sum(tens * T)\n",
        "\n",
        "\n",
        "def gwggrad(\n",
        "    constC,\n",
        "    hC1,\n",
        "    hC2,\n",
        "    T,\n",
        "    method_type=None,\n",
        "    loss_fun=None,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "):\n",
        "\n",
        "    \"\"\"Return the gradient for Gromov-Wasserstein\n",
        "    The gradient is computed as described in Proposition 2 in [1].\n",
        "    Parameters\n",
        "    ----------\n",
        "    constC : ndarray, shape (ns, nt)\n",
        "           Constant C matrix in Eq. (6)\n",
        "    hC1 : ndarray, shape (ns, ns)\n",
        "           h1(C1) matrix in Eq. (6)\n",
        "    hC2 : ndarray, shape (nt, nt)\n",
        "           h2(C) matrix in Eq. (6)\n",
        "    T : ndarray, shape (ns, nt)\n",
        "           Current value of transport matrix T\n",
        "    Optional :\n",
        "    method_type : str None defaults to brute force\n",
        "    source_integrator : Callable function that does fast matrix multplication for source graph\n",
        "    target_integrator : Callable function that does fast matrix multplication for target graph\n",
        "    Returns\n",
        "    -------\n",
        "    grad : ndarray, shape (ns, nt)\n",
        "           Gromov Wasserstein gradient\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Peyré, Gabriel, Marco Cuturi, and Justin Solomon,\n",
        "    \"Gromov-Wasserstein averaging of kernel and distance matrices.\"\n",
        "    International Conference on Machine Learning (ICML). 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    return 2 * tensor_product(\n",
        "        constC,\n",
        "        hC1,\n",
        "        hC2,\n",
        "        T,\n",
        "        method_type=method_type,\n",
        "        loss_fun=loss_fun,\n",
        "        source_integrator=source_integrator,\n",
        "        target_integrator=target_integrator,\n",
        "    )\n",
        "\n",
        "\n",
        "def gw_lp(\n",
        "    C1=None,\n",
        "    C2=None,\n",
        "    p=None,\n",
        "    q=None,\n",
        "    loss_fun=\"square_loss\",\n",
        "    alpha=1,\n",
        "    armijo=True,\n",
        "    G0=None,\n",
        "    log=True,\n",
        "    method_type=None,\n",
        "    dim: int = None,\n",
        "    source_adjacency_lists=None,\n",
        "    threshold=None,\n",
        "    target_adjacency_lists=None,\n",
        "    verbose=False,\n",
        "    source_integrator=None,\n",
        "    target_integrator=None,\n",
        "    max_iter=1000,\n",
        "    stopThr=1e-9,\n",
        "    func=None,\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the gromov-wasserstein transport between (C1,p) and (C2,q)\n",
        "    The function solves the following optimization problem:\n",
        "    .. math::\n",
        "        \\GW_Dist = \\min_T \\sum_{i,j,k,l} L(C1_{i,k},C2_{j,l})*T_{i,j}*T_{k,l}\n",
        "    Where :\n",
        "        C1 : Metric cost matrix in the source space\n",
        "        C2 : Metric cost matrix in the target space\n",
        "        p  : distribution in the source space\n",
        "        q  : distribution in the target space\n",
        "        L  : loss function to account for the misfit between the similarity matrices\n",
        "        H  : entropy\n",
        "    Parameters\n",
        "    ----------\n",
        "    C1 : ndarray, shape (ns, ns)\n",
        "         Metric cost matrix in the source space\n",
        "    C2 : ndarray, shape (nt, nt)\n",
        "         Metric costfr matrix in the target space\n",
        "    p :  ndarray, shape (ns,)\n",
        "         distribution in the source space\n",
        "    q :  ndarray, shape (nt,)\n",
        "         distribution in the target space\n",
        "    loss_fun :  string\n",
        "        loss function used for the solver\n",
        "    max_iter : int, optional\n",
        "        Max number of iterations\n",
        "    tol : float, optional\n",
        "        Stop threshold on error (>0)\n",
        "    verbose : bool, optional\n",
        "        Print information along iterations\n",
        "    log : bool, optional\n",
        "        record log if True\n",
        "    armijo : bool, optional\n",
        "        If True the step of the line-search is found via an armijo research. Else closed form is used.\n",
        "        If there is convergence issues use False.\n",
        "     G0: ndarray, shape (ns,nt), optional\n",
        "        If None the initial transport plan of the solver is pq^T.\n",
        "        Otherwise G0 must satisfy marginal constraints and will be used as initial transport of the solver.\n",
        "    The rest of the parameters are optional and only used for fast matrix vector multiplication.\n",
        "        method_type : Choose from [None, \"diffusion\", \"separator\"]\n",
        "        source_positions : (n_s, dim) location of points in d-dim Euclidean space.\n",
        "        target_positions : (n_t, dim) location of points in d-dim Euclidean space.\n",
        "        source_epsilon : parameter that controls the epsilon neighbor of source points\n",
        "        target_epsilon : parameter that controls the epsilon neighbor of target points\n",
        "        source_lambda_par : diffusion parameter for source graph.\n",
        "        target_lambda_par : diffusion parameter for target graph.\n",
        "        num_rand_features : Number of random features\n",
        "        dim : Input dimensionality of the data\n",
        "    verbose : bool, optional\n",
        "        If true returns logs/errors in each iteration\n",
        "    Returns\n",
        "    -------\n",
        "    T : ndarray, shape (ns, nt)\n",
        "        coupling between the two spaces that minimizes :\n",
        "            \\sum_{i,j,k,l} L(C1_{i,k},C2_{j,l})*T_{i,j}*T_{k,l}\n",
        "    log : dict\n",
        "        convergence information and loss\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Peyré, Gabriel, Marco Cuturi, and Justin Solomon,\n",
        "        \"Gromov-Wasserstein averaging of kernel and distance matrices.\"\n",
        "        International Conference on Machine Learning (ICML). 2016.\n",
        "    .. [2] Mémoli, Facundo. Gromov–Wasserstein distances and the\n",
        "        metric approach to object matching. Foundations of computational\n",
        "        mathematics 11.4 (2011): 417-487.\n",
        "    \"\"\"\n",
        "\n",
        "    if method_type is not None:\n",
        "        if source_integrator is None and target_integrator is None:\n",
        "            s_integrator = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=source_adjacency_lists,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold)\n",
        "            t_integrator = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=target_adjacency_lists,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold) #hardcoded\n",
        "        else:\n",
        "            s_integrator = source_integrator\n",
        "            t_integrator = target_integrator\n",
        "\n",
        "        if loss_fun == \"square_loss\":\n",
        "            constC, hC1, hC2 = init_matrix(\n",
        "                C1,\n",
        "                C2,\n",
        "                p,\n",
        "                q,\n",
        "                loss_fun,\n",
        "                method_type=method_type,\n",
        "                source_integrator=s_integrator,\n",
        "                target_integrator=t_integrator,\n",
        "            )\n",
        "        elif loss_fun == \"kl_loss\":\n",
        "            constC, hC1, hC2 = init_matrix(\n",
        "                C1, # shortest path matrices in our case\n",
        "                C2,\n",
        "                p,\n",
        "                q,\n",
        "                loss_fun,\n",
        "                method_type=method_type,\n",
        "                source_integrator=None,\n",
        "                target_integrator=t_integrator,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"incorrect loss function used\")\n",
        "\n",
        "    else:\n",
        "        s_integrator = None\n",
        "        t_integrator = None\n",
        "        constC, hC1, hC2 = init_matrix(\n",
        "            C1,\n",
        "            C2,\n",
        "            p,\n",
        "            q,\n",
        "            loss_fun,\n",
        "            method_type=method_type,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "        )\n",
        "\n",
        "\n",
        "    if method_type is None:\n",
        "        M = np.zeros((C1.shape[0], C2.shape[0]))\n",
        "    else:\n",
        "        M = np.zeros((p.shape[0], q.shape[0]))\n",
        "\n",
        "    if G0 is None:\n",
        "        G0 = p[:, None] * q[None, :]\n",
        "    else:  # check marginals\n",
        "        np.testing.assert_allclose(G0.sum(axis=1), p, atol=1e-08)\n",
        "        np.testing.assert_allclose(G0.sum(axis=0), q, atol=1e-08)\n",
        "\n",
        "    def f(G):\n",
        "        return gwloss(\n",
        "            constC,\n",
        "            hC1,\n",
        "            hC2,\n",
        "            G,\n",
        "            method_type=method_type,\n",
        "            loss_fun=loss_fun,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "        )\n",
        "\n",
        "    def df(G):\n",
        "        return gwggrad(\n",
        "            constC,\n",
        "            hC1,\n",
        "            hC2,\n",
        "            G,\n",
        "            method_type=method_type,\n",
        "            loss_fun=loss_fun,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "        )\n",
        "\n",
        "    if log:\n",
        "        res, log0 = cg(\n",
        "            a=p,\n",
        "            b=q,\n",
        "            M=M,\n",
        "            reg=alpha,\n",
        "            f=f,\n",
        "            df=df,\n",
        "            G0=G0,\n",
        "            armijo=armijo,\n",
        "            C1=C1,\n",
        "            C2=C2,\n",
        "            constC=constC,\n",
        "            log=log,\n",
        "            alpha_min=0,\n",
        "            alpha_max=1,\n",
        "            method_type=method_type,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "            verbose=verbose,\n",
        "            numItermax=max_iter,\n",
        "            stopThr=stopThr,\n",
        "        )\n",
        "        log0[\"gw_dist\"] = gwloss(\n",
        "            constC,\n",
        "            hC1,\n",
        "            hC2,\n",
        "            res,\n",
        "            method_type=method_type,\n",
        "            loss_fun=loss_fun,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "        )\n",
        "        return res, log0\n",
        "    else:\n",
        "        res = cg(\n",
        "            a=p,\n",
        "            b=q,\n",
        "            M=M,\n",
        "            reg=alpha,\n",
        "            f=f,\n",
        "            df=df,\n",
        "            G0=G0,\n",
        "            armijo=armijo,\n",
        "            C1=C1,\n",
        "            C2=C2,\n",
        "            constC=constC,\n",
        "            log=log,\n",
        "            alpha_min=0,\n",
        "            alpha_max=1,\n",
        "            method_type=method_type,\n",
        "            source_integrator=s_integrator,\n",
        "            target_integrator=t_integrator,\n",
        "            verbose=verbose,\n",
        "            numItermax=max_iter,\n",
        "            stopThr=stopThr,\n",
        "        )\n",
        "        return res"
      ],
      "metadata": {
        "id": "95o8weosBVlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as pl\n",
        "# from ot.gromov import semirelaxed_gromov_wasserstein, semirelaxed_fused_gromov_wasserstein, gromov_wasserstein, fused_gromov_wasserstein\n",
        "import networkx\n",
        "from networkx.generators.community import stochastic_block_model as sbm"
      ],
      "metadata": {
        "id": "yoKQbeM_TTF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_edge_lists(graph, weights=False):\n",
        "  adj_lists = []\n",
        "  for k, v in graph.adjacency():\n",
        "    lists = []\n",
        "    # print(k)\n",
        "    for t, w in v.items() :\n",
        "      if len(w) == 0:\n",
        "        lists.append([t, 1])\n",
        "      else :\n",
        "    # possible bug depending on the structure of w\n",
        "        lists.append([t,w])\n",
        "    # print(lists)\n",
        "    adj_lists.append(lists)\n",
        "\n",
        "  return adj_lists"
      ],
      "metadata": {
        "id": "GJd4Cjiya59y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct_edge_lists(G2)"
      ],
      "metadata": {
        "id": "q0Pc9cGkb_wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN THE ALGORITHM ON TREES"
      ],
      "metadata": {
        "id": "y0abfWcRQX_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from networkx.generators.trees import random_tree\n",
        "\n",
        "for i in [500, 1000, 2000, 2500]:\n",
        "  G2 = random_tree(n=i, seed=0)\n",
        "  G3 = random_tree(n=1, seed=42)\n",
        "  start = time.time()\n",
        "  C2 = networkx.floyd_warshall_numpy(G2)\n",
        "  C3 = networkx.floyd_warshall_numpy(G3)\n",
        "  h2 = np.ones(C2.shape[0]) / C2.shape[0]\n",
        "  h3 = np.ones(C3.shape[0]) / C3.shape[0]\n",
        "  _, log = gw_lp(C2, C3, h2, h3, log=True)\n",
        "  end = time.time()\n",
        "  gw = log['gw_dist']\n",
        "  print(f\"Time taken for baseline gw on {i} nodes is {end-start} and the distance is {gw}\")\n",
        "  del G2, G3, C2, C3, log, h2, h3"
      ],
      "metadata": {
        "id": "DOyZqV8UdPVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigger graphs\n",
        "for i in [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]:\n",
        "  G2 = random_tree(n=i, seed=0)\n",
        "  G3 = random_tree(n=1, seed=42)\n",
        "  start = time.time()\n",
        "  C2 = networkx.floyd_warshall_numpy(G2)\n",
        "  C3 = networkx.floyd_warshall_numpy(G3)\n",
        "  h2 = np.ones(C2.shape[0]) / C2.shape[0]\n",
        "  h3 = np.ones(C3.shape[0]) / C3.shape[0]\n",
        "  _, log = gw_lp(C2, C3, h2, h3, log=True)\n",
        "  end = time.time()\n",
        "  gw = log['gw_dist']\n",
        "  print(f\"Time taken for baseline gw on {i} nodes is {end-start} and the distance is {gw}\")\n",
        "  del G2, G3, C2, C3, log, h2, h3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9lEOfUEVKVy",
        "outputId": "54eb395b-839a-41a3-ce22-ec2c7425f9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for baseline gw on 3000 nodes is 194.95167183876038 and the distance is 6394.569884222222\n",
            "Time taken for baseline gw on 4000 nodes is 401.9335091114044 and the distance is 6877.147011999999\n",
            "Time taken for baseline gw on 5000 nodes is 769.647983789444 and the distance is 10339.689725039996\n",
            "Time taken for baseline gw on 6000 nodes is 1328.2349543571472 and the distance is 13561.547033722221\n",
            "Time taken for baseline gw on 7000 nodes is 2102.0649557113647 and the distance is 17092.65742036735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j,i in enumerate([500, 1000, 2000]):\n",
        "  threshold = [50, 100, 150]\n",
        "  func = lambda x : x\n",
        "  G2 = random_tree(n=i, seed=0)\n",
        "  G3 = random_tree(n=1, seed=42)\n",
        "  s_graph = construct_edge_lists(G2)\n",
        "  t_graph = construct_edge_lists(G3)\n",
        "  h2 = np.ones(i) / i\n",
        "  h3 = np.ones(i) / i\n",
        "  start_time = time.time()\n",
        "  s_tbgi = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=s_graph,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold[j])\n",
        "\n",
        "  t_tbgi = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=t_graph,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold[j])\n",
        "\n",
        "  _, log2 = gw_lp(\n",
        "        C1=None,\n",
        "        C2=None,\n",
        "        p=h2,\n",
        "        q=h3,\n",
        "        loss_fun=\"square_loss\",\n",
        "        alpha=.5,\n",
        "        armijo=False,\n",
        "        G0=None,\n",
        "        log=True,\n",
        "        method_type=\"diffusion\",\n",
        "        source_integrator=s_tbgi,\n",
        "        target_integrator=t_tbgi,\n",
        "        max_iter=200000,\n",
        "    )\n",
        "  end = time.time()\n",
        "  gw = log2['gw_dist']\n",
        "  print(f\"Time taken for baseline gw on {i} nodes is {end-start_time} and the distance is {gw}\")\n",
        "  del G2, G3, s_graph, t_graph, s_tbgi, t_tbgi, log2, h2, h3"
      ],
      "metadata": {
        "id": "2RP8392ye2h2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b4b2a9-3622-4eba-c51d-91d93452d8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for baseline gw on 500 nodes is 0.6170721054077148 and the distance is 1585.8247039999999\n",
            "Time taken for baseline gw on 1000 nodes is 2.304067611694336 and the distance is 2088.9115020000013\n",
            "Time taken for baseline gw on 2000 nodes is 6.896076917648315 and the distance is 3826.6408794999966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our method for bigger graphs\n",
        "\n",
        "for j,i in enumerate([3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]):\n",
        "  threshold = [500, 600, 700, 800, 900, 900, 900, 1000]\n",
        "  func = lambda x : x\n",
        "  G2 = random_tree(n=i, seed=0)\n",
        "  G3 = random_tree(n=1, seed=42)\n",
        "  s_graph = construct_edge_lists(G2)\n",
        "  t_graph = construct_edge_lists(G3)\n",
        "  h2 = np.ones(i) / i\n",
        "  h3 = np.ones(i) / i\n",
        "  start_time = time.time()\n",
        "  s_tbgi = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=s_graph,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold[j])\n",
        "\n",
        "  t_tbgi = TreeBasedGraphIntegrator(f_func=func, is_lambda=True, graph_adj_lists=t_graph,\n",
        "                                    tree_constructor=MinimumSpanningTreeConstructor(),\n",
        "                                    threshold=threshold[j])\n",
        "\n",
        "  _, log2 = gw_lp(\n",
        "        C1=None,\n",
        "        C2=None,\n",
        "        p=h2,\n",
        "        q=h3,\n",
        "        loss_fun=\"square_loss\",\n",
        "        alpha=.5,\n",
        "        armijo=False,\n",
        "        G0=None,\n",
        "        log=True,\n",
        "        method_type=\"diffusion\",\n",
        "        source_integrator=s_tbgi,\n",
        "        target_integrator=t_tbgi,\n",
        "        max_iter=200000,\n",
        "    )\n",
        "  end = time.time()\n",
        "  gw = log2['gw_dist']\n",
        "  print(f\"Time taken for baseline gw on {i} nodes is {end-start} and the distance is {gw}\")\n",
        "  del G2, G3, s_graph, t_graph, s_tbgi, t_tbgi, log2, h2, h3"
      ],
      "metadata": {
        "id": "38VzutafTadJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}